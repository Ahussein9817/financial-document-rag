{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ae6a4b-8ce6-4577-96da-e0d3ce5d67c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/alihussein/venv/bin/python -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e11e14b6-a442-4b0a-ac5d-dffc9e3b796b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports complete!\n",
      "Date: Thursday, February 19, 2026\n",
      "Time: 07:06 PM\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Day 3: Evaluation & Documentation\n",
    "Goal: Measure RAG system performance with ground truth evaluation\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úì Imports complete!\")\n",
    "print(f\"Date: {datetime.now().strftime('%A, %B %d, %Y')}\")\n",
    "print(f\"Time: {datetime.now().strftime('%I:%M %p')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5771d396-6bef-4b0c-84e6-6fdbe500b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì RAG system loaded!\n",
      "  Vector store: 17306 chunks\n",
      "  Retrieval: k=4\n",
      "  LLM: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load the optimized RAG system from Day 2\n",
    "\"\"\"\n",
    "\n",
    "# Load vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(\n",
    "    persist_directory='../data/vectorstore',\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úì RAG system loaded!\")\n",
    "print(f\"  Vector store: {vectorstore._collection.count()} chunks\")\n",
    "print(f\"  Retrieval: k=4\")\n",
    "print(f\"  LLM: gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3fb0b7d-381a-4ab8-bf72-7fc31da44b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Created 15 evaluation questions\n",
      "\n",
      "Breakdown:\n",
      "  Factual: 10\n",
      "  Comparative: 3\n",
      "  Trend: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EVALUATION DATASET: 15 questions with verified ground truth answers\n",
    "Categories: Factual (10), Comparative (3), Trend (2)\n",
    "\"\"\"\n",
    "\n",
    "evaluation_questions = [\n",
    "    # FACTUAL QUESTIONS (10)\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What was JPMorgan Chase total revenue in 2024?',\n",
    "        'ground_truth': '$224,532 million or $224.5 billion',\n",
    "        'source': 'JPM_10K_2024.pdf',\n",
    "        'notes': 'Look for consolidated statement of income'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What are Bank of America four main business segments?',\n",
    "        'ground_truth': 'Consumer Banking, Global Wealth & Investment Management (GWIM), Global Banking, and Global Markets',\n",
    "        'source': 'BAC_10K_2024.pdf',\n",
    "        'notes': 'Check business segment overview section'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What was Citigroup total assets at end of 2023?',\n",
    "        'ground_truth': '$2,416 billion or $2.416 trillion',\n",
    "        'source': 'C_10K_2023.pdf',\n",
    "        'notes': 'Consolidated balance sheet'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'How many employees does JPMorgan have?',\n",
    "        'ground_truth': 'Approximately 310,000+ employees',\n",
    "        'source': 'JPM_10K_2024.pdf',\n",
    "        'notes': 'Check business overview or workforce section'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What is Bank of America CET1 capital ratio in 2024?',\n",
    "        'ground_truth': '11.8% standardized approach',\n",
    "        'source': 'BAC_10K_2024.pdf',\n",
    "        'notes': 'Regulatory capital section'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What was JPMorgan net income in 2024?',\n",
    "        'ground_truth': '$57.0 billion or $57,040 million',\n",
    "        'source': 'JPM_10K_2024.pdf',\n",
    "        'notes': 'Income statement'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What is Citigroup return on equity ROE in 2024?',\n",
    "        'ground_truth': 'Approximately 8.1%',\n",
    "        'source': 'C_10K_2024.pdf',\n",
    "        'notes': 'Financial highlights or ratios section'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What was Bank of America net interest income in 2024?',\n",
    "        'ground_truth': '$56,060 million or $56.1 billion',\n",
    "        'source': 'BAC_10K_2024.pdf',\n",
    "        'notes': 'Income statement'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'How many credit cards does Bank of America have outstanding?',\n",
    "        'ground_truth': 'Over 51 million credit card accounts',\n",
    "        'source': 'BAC_10K_2024.pdf',\n",
    "        'notes': 'Consumer banking metrics'\n",
    "    },\n",
    "    {\n",
    "        'category': 'factual',\n",
    "        'question': 'What was Citigroup total revenue in 2024?',\n",
    "        'ground_truth': '$81,107 million or $81.1 billion',\n",
    "        'source': 'C_10K_2024.pdf',\n",
    "        'notes': 'Income statement, total revenues'\n",
    "    },\n",
    "    \n",
    "    # COMPARATIVE QUESTIONS (3)\n",
    "    {\n",
    "        'category': 'comparative',\n",
    "        'question': 'Which bank had the highest net income in 2024: JPMorgan, Bank of America, or Citigroup?',\n",
    "        'ground_truth': 'JPMorgan with $57.0 billion (vs BAC ~$26.5B, Citi ~$12.7B)',\n",
    "        'source': 'Multiple: JPM_10K_2024, BAC_10K_2024, C_10K_2024',\n",
    "        'notes': 'Compare net income across all three'\n",
    "    },\n",
    "    {\n",
    "        'category': 'comparative',\n",
    "        'question': 'Compare the total assets of JPMorgan and Citigroup in 2024',\n",
    "        'ground_truth': 'JPMorgan: ~$4.1 trillion, Citigroup: ~$2.4 trillion. JPMorgan is larger.',\n",
    "        'source': 'JPM_10K_2024.pdf, C_10K_2024.pdf',\n",
    "        'notes': 'Balance sheets'\n",
    "    },\n",
    "    {\n",
    "        'category': 'comparative',\n",
    "        'question': 'Which bank has more employees: JPMorgan or Bank of America?',\n",
    "        'ground_truth': 'JPMorgan with ~310,000 vs Bank of America with ~216,000',\n",
    "        'source': 'JPM_10K_2024.pdf, BAC_10K_2024.pdf',\n",
    "        'notes': 'Workforce/employee count sections'\n",
    "    },\n",
    "    \n",
    "    # TREND QUESTIONS (2)\n",
    "    {\n",
    "        'category': 'trend',\n",
    "        'question': 'How did JPMorgan revenue change from 2023 to 2024?',\n",
    "        'ground_truth': 'Increased from ~$195.6B to $224.5B (increase of ~15%)',\n",
    "        'source': 'JPM_10K_2024.pdf, JPM_10K_2023.pdf',\n",
    "        'notes': 'Multi-year income statement comparison'\n",
    "    },\n",
    "    {\n",
    "        'category': 'trend',\n",
    "        'question': 'What is the trend in Citigroup net income from 2022 to 2024?',\n",
    "        'ground_truth': 'Variable: 2022: ~$15B, 2023: ~$9.2B, 2024: ~$12.7B (declined then partially recovered)',\n",
    "        'source': 'C_10K_2024.pdf, C_10K_2023.pdf, C_10K_2022.pdf',\n",
    "        'notes': 'Three-year comparison needed'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úì Created {len(evaluation_questions)} evaluation questions\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  Factual: {len([q for q in evaluation_questions if q['category'] == 'factual'])}\")\n",
    "print(f\"  Comparative: {len([q for q in evaluation_questions if q['category'] == 'comparative'])}\")\n",
    "print(f\"  Trend: {len([q for q in evaluation_questions if q['category'] == 'trend'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ee478e-ca13-4e21-96b8-3f721b994a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAG on all test questions...\n",
      "(This will take 2-3 minutes)\n",
      "\n",
      "[1/15] What was JPMorgan Chase total revenue in 2024?...\n",
      "[2/15] What are Bank of America four main business segments?...\n",
      "[3/15] What was Citigroup total assets at end of 2023?...\n",
      "[4/15] How many employees does JPMorgan have?...\n",
      "[5/15] What is Bank of America CET1 capital ratio in 2024?...\n",
      "[6/15] What was JPMorgan net income in 2024?...\n",
      "[7/15] What is Citigroup return on equity ROE in 2024?...\n",
      "[8/15] What was Bank of America net interest income in 2024?...\n",
      "[9/15] How many credit cards does Bank of America have outstanding?...\n",
      "[10/15] What was Citigroup total revenue in 2024?...\n",
      "[11/15] Which bank had the highest net income in 2024: JPMorgan, Ban...\n",
      "[12/15] Compare the total assets of JPMorgan and Citigroup in 2024...\n",
      "[13/15] Which bank has more employees: JPMorgan or Bank of America?...\n",
      "[14/15] How did JPMorgan revenue change from 2023 to 2024?...\n",
      "[15/15] What is the trend in Citigroup net income from 2022 to 2024?...\n",
      "\n",
      "‚úì Completed RAG inference on all 15 questions!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run RAG system on all 15 questions and collect results\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running RAG on all test questions...\")\n",
    "print(\"(This will take 2-3 minutes)\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, test_case in enumerate(evaluation_questions):\n",
    "    print(f\"[{i+1}/15] {test_case['question'][:60]}...\")\n",
    "    \n",
    "    # Get RAG answer\n",
    "    answer = rag_chain.invoke(test_case['question'])\n",
    "    \n",
    "    # Get source documents\n",
    "    docs = retriever.invoke(test_case['question'])\n",
    "    \n",
    "    # Store result\n",
    "    results.append({\n",
    "        'id': i+1,\n",
    "        'category': test_case['category'],\n",
    "        'question': test_case['question'],\n",
    "        'ground_truth': test_case['ground_truth'],\n",
    "        'rag_answer': answer,\n",
    "        'sources': [\n",
    "            {\n",
    "                'file': doc.metadata.get('source_file'),\n",
    "                'ticker': doc.metadata.get('ticker'),\n",
    "                'year': doc.metadata.get('year')\n",
    "            }\n",
    "            for doc in docs\n",
    "        ],\n",
    "        'primary_source': docs[0].metadata.get('source_file') if docs else 'None',\n",
    "        'expected_source': test_case['source'],\n",
    "        'notes': test_case['notes']\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úì Completed RAG inference on all {len(results)} questions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b740feaf-dd3b-48e9-9b48-078c240dc14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MANUAL EVALUATION - Review each answer\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 1 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What was JPMorgan Chase total revenue in 2024?\n",
      "\n",
      "Ground Truth: $224,532 million or $224.5 billion\n",
      "\n",
      "RAG Answer: $224,532 million\n",
      "\n",
      "Expected Source: JPM_10K_2024.pdf\n",
      "Actual Sources: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2024.pdf', 'JPM_10K_2024.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 2 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What are Bank of America four main business segments?\n",
      "\n",
      "Ground Truth: Consumer Banking, Global Wealth & Investment Management (GWIM), Global Banking, and Global Markets\n",
      "\n",
      "RAG Answer: Consumer Banking, Global Wealth & Investment Management (GWIM), Global Banking, and Global Markets.\n",
      "\n",
      "Expected Source: BAC_10K_2024.pdf\n",
      "Actual Sources: ['BAC_10K_2024.pdf', 'BAC_10K_2022.pdf', 'JPM_10K_2023.pdf', 'BAC_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 3 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What was Citigroup total assets at end of 2023?\n",
      "\n",
      "Ground Truth: $2,416 billion or $2.416 trillion\n",
      "\n",
      "RAG Answer: $2,416,676 million\n",
      "\n",
      "Expected Source: C_10K_2023.pdf\n",
      "Actual Sources: ['C_10K_2024.pdf', 'C_10K_2023.pdf', 'C_10K_2023.pdf', 'C_10K_2022.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 4 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: How many employees does JPMorgan have?\n",
      "\n",
      "Ground Truth: Approximately 310,000+ employees\n",
      "\n",
      "RAG Answer: As of December 31, 2025, JPMorganChase had 318,512 employees globally.\n",
      "\n",
      "Expected Source: JPM_10K_2024.pdf\n",
      "Actual Sources: ['JPM_10K_2025.pdf', 'JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 5 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What is Bank of America CET1 capital ratio in 2024?\n",
      "\n",
      "Ground Truth: 11.8% standardized approach\n",
      "\n",
      "RAG Answer: The Bank of America CET1 capital ratio in 2024 is 15.8%.\n",
      "\n",
      "Expected Source: BAC_10K_2024.pdf\n",
      "Actual Sources: ['JPM_10K_2025.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf', 'C_10K_2022.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 6 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What was JPMorgan net income in 2024?\n",
      "\n",
      "Ground Truth: $57.0 billion or $57,040 million\n",
      "\n",
      "RAG Answer: JPMorgan's net income in 2024 was $48,665 million.\n",
      "\n",
      "Expected Source: JPM_10K_2024.pdf\n",
      "Actual Sources: ['JPM_10K_2024.pdf', 'JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2025.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 7 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What is Citigroup return on equity ROE in 2024?\n",
      "\n",
      "Ground Truth: Approximately 8.1%\n",
      "\n",
      "RAG Answer: 9.4%\n",
      "\n",
      "Expected Source: C_10K_2024.pdf\n",
      "Actual Sources: ['C_10K_2024.pdf', 'C_10K_2022.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 8 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What was Bank of America net interest income in 2024?\n",
      "\n",
      "Ground Truth: $56,060 million or $56.1 billion\n",
      "\n",
      "RAG Answer: $56,060 million\n",
      "\n",
      "Expected Source: BAC_10K_2024.pdf\n",
      "Actual Sources: ['BAC_10K_2024.pdf', 'BAC_10K_2024.pdf', 'BAC_10K_2022.pdf', 'BAC_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 9 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: How many credit cards does Bank of America have outstanding?\n",
      "\n",
      "Ground Truth: Over 51 million credit card accounts\n",
      "\n",
      "RAG Answer: Based on the context provided, it is not possible to determine the exact number of credit cards Bank of America has outstanding.\n",
      "\n",
      "Expected Source: BAC_10K_2024.pdf\n",
      "Actual Sources: ['BAC_10K_2022.pdf', 'BAC_10K_2024.pdf', 'BAC_10K_2023.pdf', 'BAC_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 10 (FACTUAL)\n",
      "================================================================================\n",
      "\n",
      "Q: What was Citigroup total revenue in 2024?\n",
      "\n",
      "Ground Truth: $81,107 million or $81.1 billion\n",
      "\n",
      "RAG Answer: $81.1 billion\n",
      "\n",
      "Expected Source: C_10K_2024.pdf\n",
      "Actual Sources: ['C_10K_2024.pdf', 'C_10K_2024.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 11 (COMPARATIVE)\n",
      "================================================================================\n",
      "\n",
      "Q: Which bank had the highest net income in 2024: JPMorgan, Bank of America, or Citigroup?\n",
      "\n",
      "Ground Truth: JPMorgan with $57.0 billion (vs BAC ~$26.5B, Citi ~$12.7B)\n",
      "\n",
      "RAG Answer: JPMorgan Chase had the highest net income in 2024.\n",
      "\n",
      "Expected Source: Multiple: JPM_10K_2024, BAC_10K_2024, C_10K_2024\n",
      "Actual Sources: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'C_10K_2022.pdf', 'JPM_10K_2024.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 12 (COMPARATIVE)\n",
      "================================================================================\n",
      "\n",
      "Q: Compare the total assets of JPMorgan and Citigroup in 2024\n",
      "\n",
      "Ground Truth: JPMorgan: ~$4.1 trillion, Citigroup: ~$2.4 trillion. JPMorgan is larger.\n",
      "\n",
      "RAG Answer: In 2024, JPMorgan had total assets of $41,076 million, while Citigroup's total assets are not provided in the context. Therefore, it is not possible to compare the total assets of JPMorgan and Citigroup in 2024 based on the information provided.\n",
      "\n",
      "Expected Source: JPM_10K_2024.pdf, C_10K_2024.pdf\n",
      "Actual Sources: ['JPM_10K_2024.pdf', 'JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2024.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 13 (COMPARATIVE)\n",
      "================================================================================\n",
      "\n",
      "Q: Which bank has more employees: JPMorgan or Bank of America?\n",
      "\n",
      "Ground Truth: JPMorgan with ~310,000 vs Bank of America with ~216,000\n",
      "\n",
      "RAG Answer: JPMorgan has more employees than Bank of America.\n",
      "\n",
      "Expected Source: JPM_10K_2024.pdf, BAC_10K_2024.pdf\n",
      "Actual Sources: ['JPM_10K_2025.pdf', 'JPM_10K_2024.pdf', 'BAC_10K_2024.pdf', 'BAC_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 14 (TREND)\n",
      "================================================================================\n",
      "\n",
      "Q: How did JPMorgan revenue change from 2023 to 2024?\n",
      "\n",
      "Ground Truth: Increased from ~$195.6B to $224.5B (increase of ~15%)\n",
      "\n",
      "RAG Answer: JPMorgan revenue increased from $158.1 billion in 2023 to $165.1 billion in 2024.\n",
      "\n",
      "Expected Source: JPM_10K_2024.pdf, JPM_10K_2023.pdf\n",
      "Actual Sources: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Question 15 (TREND)\n",
      "================================================================================\n",
      "\n",
      "Q: What is the trend in Citigroup net income from 2022 to 2024?\n",
      "\n",
      "Ground Truth: Variable: 2022: ~$15B, 2023: ~$9.2B, 2024: ~$12.7B (declined then partially recovered)\n",
      "\n",
      "RAG Answer: The trend in Citigroup net income from 2022 to 2024 is a decrease.\n",
      "\n",
      "Expected Source: C_10K_2024.pdf, C_10K_2023.pdf, C_10K_2022.pdf\n",
      "Actual Sources: ['C_10K_2022.pdf', 'C_10K_2024.pdf', 'C_10K_2024.pdf', 'C_10K_2023.pdf']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Display questions side-by-side for manual evaluation\n",
    "Rate each as: Correct, Partial, Incorrect\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MANUAL EVALUATION - Review each answer\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for r in results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {r['id']} ({r['category'].upper()})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nQ: {r['question']}\")\n",
    "    print(f\"\\nGround Truth: {r['ground_truth']}\")\n",
    "    print(f\"\\nRAG Answer: {r['rag_answer']}\")\n",
    "    print(f\"\\nExpected Source: {r['expected_source']}\")\n",
    "    print(f\"Actual Sources: {[s['file'] for s in r['sources']]}\")\n",
    "    print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b01a9d0c-49c7-4cd9-9e47-aa77022be225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Manual scores added!\n",
      "\n",
      "Scoring guide:\n",
      "  1.0 = Correct (answer matches ground truth)\n",
      "  0.5 = Partial (answer partially correct or lacks detail)\n",
      "  0.0 = Incorrect (answer is wrong)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "After reviewing above, manually score each question\n",
    "Correct = 1, Partial = 0.5, Incorrect = 0\n",
    "\"\"\"\n",
    "\n",
    "# MANUALLY UPDATE THESE SCORES based on your review above\n",
    "manual_scores = [\n",
    "    1.0,  # Q1: Correct - exact match ($224,532 million) ‚úì\n",
    "    1.0,  # Q2: Correct - all 4 segments listed perfectly ‚úì\n",
    "    1.0,  # Q3: Correct - $2,416,676M = $2,416B (same number) ‚úì\n",
    "    1.0,  # Q4: Correct - 318,512 employees (close to 310K+) ‚úì\n",
    "    0.0,  # Q5: INCORRECT - said 15.8%, should be 11.8% + wrong sources ‚úó\n",
    "    0.0,  # Q6: INCORRECT - said $48.7B, should be $57.0B ‚úó\n",
    "    0.5,  # Q7: Partial - said 9.4%, ground truth is 8.1% (close but off) ‚ö†\n",
    "    1.0,  # Q8: Correct - exact match ($56,060 million) ‚úì\n",
    "    0.0,  # Q9: INCORRECT - couldn't find answer (should be 51M+) ‚úó\n",
    "    1.0,  # Q10: Correct - $81.1 billion ‚úì\n",
    "    0.5,  # Q11: Partial - correct (JPM highest) but no numbers provided ‚ö†\n",
    "    0.0,  # Q12: INCORRECT - wrong units ($41M vs $4.1T) + couldn't compare ‚úó\n",
    "    0.5,  # Q13: Partial - correct direction but no employee counts ‚ö†\n",
    "    0.0,  # Q14: INCORRECT - completely wrong revenue numbers ‚úó\n",
    "    0.5   # Q15: Partial - captured decline but missed the recovery ‚ö†\n",
    "]\n",
    "\n",
    "# Add scores to results\n",
    "for i, score in enumerate(manual_scores):\n",
    "    results[i]['score'] = score\n",
    "    results[i]['score_label'] = 'Correct' if score == 1.0 else ('Partial' if score == 0.5 else 'Incorrect')\n",
    "\n",
    "print(\"‚úì Manual scores added!\")\n",
    "print(f\"\\nScoring guide:\")\n",
    "print(\"  1.0 = Correct (answer matches ground truth)\")\n",
    "print(\"  0.5 = Partial (answer partially correct or lacks detail)\")\n",
    "print(\"  0.0 = Incorrect (answer is wrong)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e29c4bd1-e85e-4dba-931a-3f599cfc6a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üìä Overall Accuracy: 53.3%\n",
      "   (8.0 / 15 questions correct)\n",
      "\n",
      "üìà Accuracy by Category:\n",
      "             Accuracy (%)  Count\n",
      "category                        \n",
      "comparative     33.333333      3\n",
      "factual         65.000000     10\n",
      "trend           25.000000      2\n",
      "\n",
      "‚úÖ Correct: 6 questions\n",
      "‚ö†Ô∏è  Partial: 4 questions\n",
      "‚ùå Incorrect: 5 questions\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Calculate overall and per-category accuracy\n",
    "\"\"\"\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = df_results['score'].mean() * 100\n",
    "\n",
    "# Per-category accuracy\n",
    "category_accuracy = df_results.groupby('category')['score'].agg(['mean', 'count'])\n",
    "category_accuracy['mean'] = category_accuracy['mean'] * 100\n",
    "category_accuracy.columns = ['Accuracy (%)', 'Count']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Overall Accuracy: {overall_accuracy:.1f}%\")\n",
    "print(f\"   ({df_results['score'].sum():.1f} / {len(df_results)} questions correct)\")\n",
    "\n",
    "print(f\"\\nüìà Accuracy by Category:\")\n",
    "print(category_accuracy)\n",
    "\n",
    "print(f\"\\n‚úÖ Correct: {len(df_results[df_results['score'] == 1.0])} questions\")\n",
    "print(f\"‚ö†Ô∏è  Partial: {len(df_results[df_results['score'] == 0.5])} questions\")\n",
    "print(f\"‚ùå Incorrect: {len(df_results[df_results['score'] == 0.0])} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5794d91e-d65e-4d91-92f6-02e6614afaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FAILURE MODE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Found 9 questions with issues:\n",
      "\n",
      "\n",
      "============================================================\n",
      "Question 5: Incorrect\n",
      "============================================================\n",
      "Q: What is Bank of America CET1 capital ratio in 2024?\n",
      "Expected: 11.8% standardized approach\n",
      "Got: The Bank of America CET1 capital ratio in 2024 is 15.8%.\n",
      "Category: factual\n",
      "Sources used: ['JPM_10K_2025.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf', 'C_10K_2022.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 6: Incorrect\n",
      "============================================================\n",
      "Q: What was JPMorgan net income in 2024?\n",
      "Expected: $57.0 billion or $57,040 million\n",
      "Got: JPMorgan's net income in 2024 was $48,665 million.\n",
      "Category: factual\n",
      "Sources used: ['JPM_10K_2024.pdf', 'JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2025.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 7: Partial\n",
      "============================================================\n",
      "Q: What is Citigroup return on equity ROE in 2024?\n",
      "Expected: Approximately 8.1%\n",
      "Got: 9.4%\n",
      "Category: factual\n",
      "Sources used: ['C_10K_2024.pdf', 'C_10K_2022.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 9: Incorrect\n",
      "============================================================\n",
      "Q: How many credit cards does Bank of America have outstanding?\n",
      "Expected: Over 51 million credit card accounts\n",
      "Got: Based on the context provided, it is not possible to determine the exact number of credit cards Bank of America has outstanding.\n",
      "Category: factual\n",
      "Sources used: ['BAC_10K_2022.pdf', 'BAC_10K_2024.pdf', 'BAC_10K_2023.pdf', 'BAC_10K_2023.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 11: Partial\n",
      "============================================================\n",
      "Q: Which bank had the highest net income in 2024: JPMorgan, Bank of America, or Citigroup?\n",
      "Expected: JPMorgan with $57.0 billion (vs BAC ~$26.5B, Citi ~$12.7B)\n",
      "Got: JPMorgan Chase had the highest net income in 2024.\n",
      "Category: comparative\n",
      "Sources used: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'C_10K_2022.pdf', 'JPM_10K_2024.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 12: Incorrect\n",
      "============================================================\n",
      "Q: Compare the total assets of JPMorgan and Citigroup in 2024\n",
      "Expected: JPMorgan: ~$4.1 trillion, Citigroup: ~$2.4 trillion. JPMorgan is larger.\n",
      "Got: In 2024, JPMorgan had total assets of $41,076 million, while Citigroup's total assets are not provided in the context. Therefore, it is not possible to compare the total assets of JPMorgan and Citigroup in 2024 based on the information provided.\n",
      "Category: comparative\n",
      "Sources used: ['JPM_10K_2024.pdf', 'JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2024.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 13: Partial\n",
      "============================================================\n",
      "Q: Which bank has more employees: JPMorgan or Bank of America?\n",
      "Expected: JPMorgan with ~310,000 vs Bank of America with ~216,000\n",
      "Got: JPMorgan has more employees than Bank of America.\n",
      "Category: comparative\n",
      "Sources used: ['JPM_10K_2025.pdf', 'JPM_10K_2024.pdf', 'BAC_10K_2024.pdf', 'BAC_10K_2023.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 14: Incorrect\n",
      "============================================================\n",
      "Q: How did JPMorgan revenue change from 2023 to 2024?\n",
      "Expected: Increased from ~$195.6B to $224.5B (increase of ~15%)\n",
      "Got: JPMorgan revenue increased from $158.1 billion in 2023 to $165.1 billion in 2024.\n",
      "Category: trend\n",
      "Sources used: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2023.pdf', 'JPM_10K_2023.pdf']\n",
      "\n",
      "============================================================\n",
      "Question 15: Partial\n",
      "============================================================\n",
      "Q: What is the trend in Citigroup net income from 2022 to 2024?\n",
      "Expected: Variable: 2022: ~$15B, 2023: ~$9.2B, 2024: ~$12.7B (declined then partially recovered)\n",
      "Got: The trend in Citigroup net income from 2022 to 2024 is a decrease.\n",
      "Category: trend\n",
      "Sources used: ['C_10K_2022.pdf', 'C_10K_2024.pdf', 'C_10K_2024.pdf', 'C_10K_2023.pdf']\n",
      "\n",
      "\n",
      "============================================================\n",
      "COMMON FAILURE PATTERNS\n",
      "============================================================\n",
      "\n",
      "\n",
      "Based on evaluation, common failure modes include:\n",
      "\n",
      "1. **Numerical Precision** (3 questions affected: Q5, Q6, Q7)\n",
      "   - Challenge: Inconsistent units ($M vs $B vs $T), rounding differences\n",
      "   - Example: Q5 asked for 11.8%, system said 15.8%\n",
      "   - Why: Retrieved chunks with different metrics or time periods\n",
      "   \n",
      "2. **Cross-Document Synthesis** (2 questions affected: Q11, Q13)\n",
      "   - Challenge: Comparing data across multiple companies\n",
      "   - Example: Q12 \"Compare JPM and Citi assets\" ‚Üí Only retrieved JPM data\n",
      "   - Why: Vector search prioritizes single company; lacks multi-document reasoning\n",
      "   \n",
      "3. **Multi-Year Trends** (2 questions affected: Q14, Q15)\n",
      "   - Challenge: Analyzing changes across fiscal years\n",
      "   - Example: Q14 \"JPM revenue 2023-2024?\" ‚Üí Wrong numbers retrieved\n",
      "   - Why: Chunks from single year dominate retrieval; temporal reasoning weak\n",
      "   \n",
      "4. **Missing Context** (2 questions affected: Q9, Q12)\n",
      "   - Challenge: Information split across multiple chunks/sections\n",
      "   - Example: Q9 \"How many credit cards?\" ‚Üí Couldn't find answer\n",
      "   - Why: Relevant data not captured in any retrieved chunk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Analyze incorrect/partial answers to identify failure patterns\n",
    "\"\"\"\n",
    "\n",
    "# Get incorrect or partial answers\n",
    "failures = df_results[df_results['score'] < 1.0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FAILURE MODE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(failures) == 0:\n",
    "    print(\"\\nüéâ No failures! All questions answered correctly!\")\n",
    "else:\n",
    "    print(f\"\\nFound {len(failures)} questions with issues:\\n\")\n",
    "    \n",
    "    for idx, row in failures.iterrows():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Question {row['id']}: {row['score_label']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Q: {row['question']}\")\n",
    "        print(f\"Expected: {row['ground_truth']}\")\n",
    "        print(f\"Got: {row['rag_answer']}\")\n",
    "        print(f\"Category: {row['category']}\")\n",
    "        print(f\"Sources used: {[s['file'] for s in row['sources']]}\")\n",
    "\n",
    "# Common failure patterns\n",
    "print(f\"\\n\\n{'='*60}\")\n",
    "print(\"COMMON FAILURE PATTERNS\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "failure_modes = \"\"\"\n",
    "Based on evaluation, common failure modes include:\n",
    "\n",
    "1. **Numerical Precision** (3 questions affected: Q5, Q6, Q7)\n",
    "   - Challenge: Inconsistent units ($M vs $B vs $T), rounding differences\n",
    "   - Example: Q5 asked for 11.8%, system said 15.8%\n",
    "   - Why: Retrieved chunks with different metrics or time periods\n",
    "   \n",
    "2. **Cross-Document Synthesis** (2 questions affected: Q11, Q13)\n",
    "   - Challenge: Comparing data across multiple companies\n",
    "   - Example: Q12 \"Compare JPM and Citi assets\" ‚Üí Only retrieved JPM data\n",
    "   - Why: Vector search prioritizes single company; lacks multi-document reasoning\n",
    "   \n",
    "3. **Multi-Year Trends** (2 questions affected: Q14, Q15)\n",
    "   - Challenge: Analyzing changes across fiscal years\n",
    "   - Example: Q14 \"JPM revenue 2023-2024?\" ‚Üí Wrong numbers retrieved\n",
    "   - Why: Chunks from single year dominate retrieval; temporal reasoning weak\n",
    "   \n",
    "4. **Missing Context** (2 questions affected: Q9, Q12)\n",
    "   - Challenge: Information split across multiple chunks/sections\n",
    "   - Example: Q9 \"How many credit cards?\" ‚Üí Couldn't find answer\n",
    "   - Why: Relevant data not captured in any retrieved chunk\n",
    "\"\"\"\n",
    "\n",
    "print(failure_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7578895-651d-4092-b941-c4861830f6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Basic evaluation results saved!\n",
      "  - evaluation_results.csv\n",
      "  - evaluation_results.json\n",
      "  - evaluation_summary_basic.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save evaluation results for documentation\n",
    "\"\"\"\n",
    "\n",
    "# Save detailed results\n",
    "df_results.to_csv('../data/processed/evaluation_results.csv', index=False)\n",
    "\n",
    "# Save as JSON with full details\n",
    "with open('../data/processed/evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# Save summary metrics (basic version - will update after recall)\n",
    "summary_basic = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'total_questions': len(df_results),\n",
    "    'overall_accuracy': float(overall_accuracy),\n",
    "    'category_accuracy': category_accuracy.to_dict(),\n",
    "    'correct_count': int(len(df_results[df_results['score'] == 1.0])),\n",
    "    'partial_count': int(len(df_results[df_results['score'] == 0.5])),\n",
    "    'incorrect_count': int(len(df_results[df_results['score'] == 0.0]))\n",
    "}\n",
    "\n",
    "with open('../data/processed/evaluation_summary_basic.json', 'w') as f:\n",
    "    json.dump(summary_basic, f, indent=2)\n",
    "\n",
    "print(\"‚úì Basic evaluation results saved!\")\n",
    "print(\"  - evaluation_results.csv\")\n",
    "print(\"  - evaluation_results.json\")\n",
    "print(\"  - evaluation_summary_basic.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deea037-9521-4fc2-87d3-27cc44f4c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluation over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6300fa0-b50c-491b-b000-c5279b5b2d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RETRIEVAL RECALL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä RETRIEVAL RECALL METRICS\n",
      "============================================================\n",
      "\n",
      "Overall Recall@4: 86.7%\n",
      "  (13/15 questions had correct source retrieved)\n",
      "\n",
      "üìà Recall by Category:\n",
      "             Recall (%)  Count\n",
      "category                      \n",
      "comparative   66.666667      3\n",
      "factual       90.000000     10\n",
      "trend        100.000000      2\n",
      "\n",
      "üîç DIAGNOSTIC ANALYSIS\n",
      "============================================================\n",
      "\n",
      "When retrieval SUCCEEDS (correct source found):\n",
      "  Answer accuracy: 57.7%\n",
      "  Questions: 13\n",
      "\n",
      "When retrieval FAILS (correct source NOT found):\n",
      "  Answer accuracy: 25.0%\n",
      "  Questions: 2\n",
      "\n",
      "‚ùå RETRIEVAL FAILURES (2 questions):\n",
      "============================================================\n",
      "\n",
      "Q5: What is Bank of America CET1 capital ratio in 2024?...\n",
      "  Expected: BAC_10K_2024.pdf\n",
      "  Retrieved: ['JPM_10K_2025.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf', 'C_10K_2022.pdf']\n",
      "  Answer Score: 0.0\n",
      "\n",
      "Q11: Which bank had the highest net income in 2024: JPMorgan, Ban...\n",
      "  Expected: Multiple: JPM_10K_2024, BAC_10K_2024, C_10K_2024\n",
      "  Retrieved: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'C_10K_2022.pdf', 'JPM_10K_2024.pdf']\n",
      "  Answer Score: 0.5\n",
      "\n",
      "üí° KEY INSIGHT:\n",
      "============================================================\n",
      "‚úì Retrieval recall is GOOD (‚â•70%)\n",
      "‚ö†Ô∏è  But accuracy is still low\n",
      "‚Üí PRIORITY: Fix generation (better prompts, GPT-4, re-ranking)\n",
      "\n",
      "‚úì Recall analysis saved: data/processed/retrieval_recall_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RETRIEVAL RECALL ANALYSIS\n",
    "Measure: Did the retriever find the right source document?\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RETRIEVAL RECALL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_retrieval_recall_for_question(question, expected_source):\n",
    "    \"\"\"\n",
    "    Check if the expected source document was in the retrieved chunks\n",
    "    \"\"\"\n",
    "    # Get retrieved documents\n",
    "    docs = retriever.invoke(question)\n",
    "    retrieved_sources = [doc.metadata.get('source_file') for doc in docs]\n",
    "    \n",
    "    # Check if expected source is in retrieved docs\n",
    "    # Handle multiple expected sources (e.g., \"JPM_10K_2024.pdf, BAC_10K_2024.pdf\")\n",
    "    expected_files = [s.strip() for s in expected_source.split(',')]\n",
    "    \n",
    "    found = any(exp in retrieved_sources for exp in expected_files)\n",
    "    \n",
    "    return {\n",
    "        'found': found,\n",
    "        'expected': expected_source,\n",
    "        'retrieved': retrieved_sources\n",
    "    }\n",
    "\n",
    "# Calculate recall for all questions\n",
    "recall_results = []\n",
    "\n",
    "for r in results:\n",
    "    recall_info = calculate_retrieval_recall_for_question(\n",
    "        r['question'], \n",
    "        r['expected_source']\n",
    "    )\n",
    "    recall_results.append({\n",
    "        'question_id': r['id'],\n",
    "        'question': r['question'],\n",
    "        'category': r['category'],\n",
    "        'recall_success': recall_info['found'],\n",
    "        'expected_source': recall_info['expected'],\n",
    "        'retrieved_sources': recall_info['retrieved'],\n",
    "        'answer_score': r['score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_recall = pd.DataFrame(recall_results)\n",
    "\n",
    "# Calculate metrics\n",
    "overall_recall = df_recall['recall_success'].mean() * 100\n",
    "\n",
    "print(f\"\\nüìä RETRIEVAL RECALL METRICS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\\nOverall Recall@4: {overall_recall:.1f}%\")\n",
    "print(f\"  ({df_recall['recall_success'].sum()}/{len(df_recall)} questions had correct source retrieved)\")\n",
    "\n",
    "# Recall by category\n",
    "recall_by_category = df_recall.groupby('category')['recall_success'].agg(['mean', 'count'])\n",
    "recall_by_category['mean'] = recall_by_category['mean'] * 100\n",
    "recall_by_category.columns = ['Recall (%)', 'Count']\n",
    "\n",
    "print(f\"\\nüìà Recall by Category:\")\n",
    "print(recall_by_category)\n",
    "\n",
    "# CRITICAL INSIGHT: Accuracy conditional on retrieval\n",
    "print(f\"\\nüîç DIAGNOSTIC ANALYSIS\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Questions with good retrieval\n",
    "good_retrieval = df_recall[df_recall['recall_success'] == True]\n",
    "bad_retrieval = df_recall[df_recall['recall_success'] == False]\n",
    "\n",
    "if len(good_retrieval) > 0:\n",
    "    accuracy_with_good_retrieval = good_retrieval['answer_score'].mean() * 100\n",
    "    print(f\"\\nWhen retrieval SUCCEEDS (correct source found):\")\n",
    "    print(f\"  Answer accuracy: {accuracy_with_good_retrieval:.1f}%\")\n",
    "    print(f\"  Questions: {len(good_retrieval)}\")\n",
    "else:\n",
    "    print(f\"\\nNo questions had successful retrieval!\")\n",
    "\n",
    "if len(bad_retrieval) > 0:\n",
    "    accuracy_with_bad_retrieval = bad_retrieval['answer_score'].mean() * 100\n",
    "    print(f\"\\nWhen retrieval FAILS (correct source NOT found):\")\n",
    "    print(f\"  Answer accuracy: {accuracy_with_bad_retrieval:.1f}%\")\n",
    "    print(f\"  Questions: {len(bad_retrieval)}\")\n",
    "\n",
    "# Show retrieval failures\n",
    "print(f\"\\n‚ùå RETRIEVAL FAILURES ({len(bad_retrieval)} questions):\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "for idx, row in bad_retrieval.iterrows():\n",
    "    print(f\"\\nQ{row['question_id']}: {row['question'][:60]}...\")\n",
    "    print(f\"  Expected: {row['expected_source']}\")\n",
    "    print(f\"  Retrieved: {row['retrieved_sources']}\")\n",
    "    print(f\"  Answer Score: {row['answer_score']}\")\n",
    "\n",
    "# Key insight\n",
    "print(f\"\\nüí° KEY INSIGHT:\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "if overall_recall < 70:\n",
    "    print(\"‚ö†Ô∏è  Retrieval recall is LOW (<70%)\")\n",
    "    print(\"‚Üí PRIORITY: Fix retrieval/chunking/embeddings FIRST\")\n",
    "    print(\"‚Üí Improving LLM won't help if chunks are wrong!\")\n",
    "elif overall_recall >= 70 and overall_accuracy < 70:\n",
    "    print(\"‚úì Retrieval recall is GOOD (‚â•70%)\")\n",
    "    print(\"‚ö†Ô∏è  But accuracy is still low\")\n",
    "    print(\"‚Üí PRIORITY: Fix generation (better prompts, GPT-4, re-ranking)\")\n",
    "else:\n",
    "    print(\"‚úì Both retrieval and generation performing reasonably well\")\n",
    "    print(\"‚Üí Focus on edge cases and advanced techniques\")\n",
    "\n",
    "# Save recall analysis\n",
    "df_recall.to_csv('../data/processed/retrieval_recall_analysis.csv', index=False)\n",
    "print(f\"\\n‚úì Recall analysis saved: data/processed/retrieval_recall_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c6c602f-9757-4fde-a89a-40082b354239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Comprehensive evaluation saved!\n",
      "  Files created:\n",
      "    - evaluation_results_complete.csv\n",
      "    - evaluation_complete.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save comprehensive evaluation results including recall metrics\n",
    "\"\"\"\n",
    "\n",
    "# Merge results with recall\n",
    "df_results_complete = df_results.copy()\n",
    "df_results_complete['recall_success'] = df_recall['recall_success'].values\n",
    "df_results_complete['retrieved_sources'] = df_recall['retrieved_sources'].values\n",
    "\n",
    "# Save to CSV\n",
    "df_results_complete.to_csv('../data/processed/evaluation_results_complete.csv', index=False)\n",
    "\n",
    "# Save comprehensive JSON\n",
    "with open('../data/processed/evaluation_complete.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'summary': {\n",
    "            'overall_accuracy': float(overall_accuracy),\n",
    "            'overall_recall_at_4': float(overall_recall),\n",
    "            'accuracy_with_good_retrieval': float(accuracy_with_good_retrieval) if len(good_retrieval) > 0 else None,\n",
    "            'accuracy_with_bad_retrieval': float(accuracy_with_bad_retrieval) if len(bad_retrieval) > 0 else None,\n",
    "            'total_questions': len(df_results),\n",
    "            'correct_count': int(len(df_results[df_results['score'] == 1.0])),\n",
    "            'partial_count': int(len(df_results[df_results['score'] == 0.5])),\n",
    "            'incorrect_count': int(len(df_results[df_results['score'] == 0.0])),\n",
    "            'retrieval_success_count': int(df_recall['recall_success'].sum()),\n",
    "            'retrieval_failure_count': int((~df_recall['recall_success']).sum())\n",
    "        },\n",
    "        'category_breakdown': {\n",
    "            'accuracy': category_accuracy.to_dict(),\n",
    "            'recall': recall_by_category.to_dict()\n",
    "        },\n",
    "        'questions_and_scores': results,\n",
    "        'recall_analysis': recall_results\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"‚úì Comprehensive evaluation saved!\")\n",
    "print(\"  Files created:\")\n",
    "print(\"    - evaluation_results_complete.csv\")\n",
    "print(\"    - evaluation_complete.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4157c8cf-204f-42f8-9b7c-dca0f165eacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üéâ DAY 3 EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "üìä Final Metrics:\n",
      "  ‚Ä¢ Overall Accuracy: 53.3%\n",
      "  ‚Ä¢ Retrieval Recall@4: 86.7%\n",
      "  ‚Ä¢ Test Questions: 15\n",
      "  ‚Ä¢ Correct: 6\n",
      "  ‚Ä¢ Partial: 4\n",
      "  ‚Ä¢ Incorrect: 5\n",
      "\n",
      "üìÅ Outputs Created:\n",
      "  ‚úì evaluation_results.csv\n",
      "  ‚úì evaluation_results.json\n",
      "  ‚úì evaluation_results_complete.csv\n",
      "  ‚úì evaluation_complete.json\n",
      "  ‚úì retrieval_recall_analysis.csv\n",
      "  ‚úì evaluation_accuracy.png (from earlier)\n",
      "  ‚úì question_performance.png (from earlier)\n",
      "  ‚úì failure_patterns.png (from earlier)\n",
      "\n",
      "üîç Key Findings:\n",
      "  ‚Ä¢ Retrieval is GOOD (86.7%)\n",
      "  ‚Ä¢ Focus improvements on generation (GPT-4, re-ranking)\n",
      "\n",
      "‚úÖ Ready for:\n",
      "  ‚Üí Update README with final numbers\n",
      "  ‚Üí Update evaluation_report.md\n",
      "  ‚Üí Commit to GitHub\n",
      "  ‚Üí Move to Day 4 tomorrow\n",
      "\n",
      "üöÄ Next: Commit everything to GitHub!\n",
      "   git add .\n",
      "   git commit -m \"Day 3 complete: Evaluation with retrieval recall\"\n",
      "   git push\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FINAL SUMMARY - Day 3 Evaluation Complete\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéâ DAY 3 EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Final Metrics:\")\n",
    "print(f\"  ‚Ä¢ Overall Accuracy: {overall_accuracy:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Retrieval Recall@4: {overall_recall:.1f}%\")\n",
    "print(f\"  ‚Ä¢ Test Questions: {len(df_results)}\")\n",
    "print(f\"  ‚Ä¢ Correct: {len(df_results[df_results['score'] == 1.0])}\")\n",
    "print(f\"  ‚Ä¢ Partial: {len(df_results[df_results['score'] == 0.5])}\")\n",
    "print(f\"  ‚Ä¢ Incorrect: {len(df_results[df_results['score'] == 0.0])}\")\n",
    "\n",
    "print(f\"\\nüìÅ Outputs Created:\")\n",
    "print(f\"  ‚úì evaluation_results.csv\")\n",
    "print(f\"  ‚úì evaluation_results.json\")\n",
    "print(f\"  ‚úì evaluation_results_complete.csv\")\n",
    "print(f\"  ‚úì evaluation_complete.json\")\n",
    "print(f\"  ‚úì retrieval_recall_analysis.csv\")\n",
    "print(f\"  ‚úì evaluation_accuracy.png (from earlier)\")\n",
    "print(f\"  ‚úì question_performance.png (from earlier)\")\n",
    "print(f\"  ‚úì failure_patterns.png (from earlier)\")\n",
    "\n",
    "print(f\"\\nüîç Key Findings:\")\n",
    "if overall_recall >= 70:\n",
    "    print(f\"  ‚Ä¢ Retrieval is GOOD ({overall_recall:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Focus improvements on generation (GPT-4, re-ranking)\")\n",
    "else:\n",
    "    print(f\"  ‚Ä¢ Retrieval needs work ({overall_recall:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Focus improvements on search (hybrid, chunking)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Ready for:\")\n",
    "print(f\"  ‚Üí Update README with final numbers\")\n",
    "print(f\"  ‚Üí Update evaluation_report.md\")\n",
    "print(f\"  ‚Üí Commit to GitHub\")\n",
    "print(f\"  ‚Üí Move to Day 4 tomorrow\")\n",
    "\n",
    "print(f\"\\nüöÄ Next: Commit everything to GitHub!\")\n",
    "print(f\"   git add .\")\n",
    "print(f\"   git commit -m \\\"Day 3 complete: Evaluation with retrieval recall\\\"\")\n",
    "print(f\"   git push\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea9edb49-8033-477e-b05c-eb73893c11e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä KEY EVALUATION INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ OVERALL PERFORMANCE\n",
      "   ‚Ä¢ Accuracy: 53.3% (8/15 questions)\n",
      "   ‚Ä¢ Best Category: factual (65.0%)\n",
      "   ‚Ä¢ Worst Category: trend (25.0%)\n",
      "\n",
      "2Ô∏è‚É£ STRENGTHS\n",
      "   ‚úì Excellent at factual lookups (single company, single year)\n",
      "   ‚úì Correct source retrieval for most questions\n",
      "   ‚úì Precise numerical answers when data is clear\n",
      "\n",
      "3Ô∏è‚É£ WEAKNESSES\n",
      "   ‚úó Cross-document comparisons (comparative questions)\n",
      "   ‚úó Multi-year trend analysis (requires 2-3 years of data)\n",
      "   ‚úó Numerical precision issues (different units, rounding)\n",
      "   ‚úó Missing context when info spans multiple chunks\n",
      "\n",
      "4Ô∏è‚É£ SPECIFIC FAILURE EXAMPLES\n",
      "\n",
      "   Question 5: What is Bank of America CET1 capital ratio in 2024?...\n",
      "   ‚Ä¢ Expected: 11.8% standardized approach...\n",
      "   ‚Ä¢ Got: The Bank of America CET1 capital ratio in 2024 is 15.8%....\n",
      "   ‚Ä¢ Issue: Numerical precision or missing context\n",
      "\n",
      "   Question 6: What was JPMorgan net income in 2024?...\n",
      "   ‚Ä¢ Expected: $57.0 billion or $57,040 million...\n",
      "   ‚Ä¢ Got: JPMorgan's net income in 2024 was $48,665 million....\n",
      "   ‚Ä¢ Issue: Numerical precision or missing context\n",
      "\n",
      "   Question 7: What is Citigroup return on equity ROE in 2024?...\n",
      "   ‚Ä¢ Expected: Approximately 8.1%...\n",
      "   ‚Ä¢ Got: 9.4%...\n",
      "   ‚Ä¢ Issue: Numerical precision or missing context\n",
      "\n",
      "5Ô∏è‚É£ RECOMMENDATIONS FOR IMPROVEMENT\n",
      "   ‚Üí Increase k to 5-6 for comparative/trend questions\n",
      "   ‚Üí Add post-processing to normalize numerical formats\n",
      "   ‚Üí Consider larger chunk sizes (1000 chars) for better context\n",
      "   ‚Üí Implement query routing (detect comparative vs factual)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate key insights from evaluation\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä KEY EVALUATION INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£ OVERALL PERFORMANCE\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {overall_accuracy:.1f}% ({df_results['score'].sum():.0f}/15 questions)\")\n",
    "print(f\"   ‚Ä¢ Best Category: {category_accuracy['Accuracy (%)'].idxmax()} ({category_accuracy['Accuracy (%)'].max():.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Worst Category: {category_accuracy['Accuracy (%)'].idxmin()} ({category_accuracy['Accuracy (%)'].min():.1f}%)\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£ STRENGTHS\")\n",
    "print(f\"   ‚úì Excellent at factual lookups (single company, single year)\")\n",
    "print(f\"   ‚úì Correct source retrieval for most questions\")\n",
    "print(f\"   ‚úì Precise numerical answers when data is clear\")\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ WEAKNESSES\")\n",
    "print(f\"   ‚úó Cross-document comparisons (comparative questions)\")\n",
    "print(f\"   ‚úó Multi-year trend analysis (requires 2-3 years of data)\")\n",
    "print(f\"   ‚úó Numerical precision issues (different units, rounding)\")\n",
    "print(f\"   ‚úó Missing context when info spans multiple chunks\")\n",
    "\n",
    "print(f\"\\n4Ô∏è‚É£ SPECIFIC FAILURE EXAMPLES\")\n",
    "\n",
    "failures = df_results[df_results['score'] < 1.0]\n",
    "for idx, row in failures.head(3).iterrows():\n",
    "    print(f\"\\n   Question {row['id']}: {row['question'][:60]}...\")\n",
    "    print(f\"   ‚Ä¢ Expected: {row['ground_truth'][:70]}...\")\n",
    "    print(f\"   ‚Ä¢ Got: {row['rag_answer'][:70]}...\")\n",
    "    print(f\"   ‚Ä¢ Issue: \", end=\"\")\n",
    "    if row['category'] == 'comparative':\n",
    "        print(\"Cross-document synthesis needed\")\n",
    "    elif row['category'] == 'trend':\n",
    "        print(\"Multi-year data required\")\n",
    "    else:\n",
    "        print(\"Numerical precision or missing context\")\n",
    "\n",
    "print(f\"\\n5Ô∏è‚É£ RECOMMENDATIONS FOR IMPROVEMENT\")\n",
    "print(f\"   ‚Üí Increase k to 5-6 for comparative/trend questions\")\n",
    "print(f\"   ‚Üí Add post-processing to normalize numerical formats\")\n",
    "print(f\"   ‚Üí Consider larger chunk sizes (1000 chars) for better context\")\n",
    "print(f\"   ‚Üí Implement query routing (detect comparative vs factual)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d89e912-142d-48c8-9b00-1fcfad8fea55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Evaluation report saved: outputs/evaluation_report.md\n",
      "\n",
      "================================================================================\n",
      "üìÑ REPORT PREVIEW\n",
      "================================================================================\n",
      "\n",
      "# RAG System Evaluation Report\n",
      "Generated: February 19, 2026 at 07:21 PM\n",
      "\n",
      "## Executive Summary\n",
      "- **Overall Accuracy:** 53.3%\n",
      "- **Total Questions:** 15\n",
      "- **Correct Answers:** 6\n",
      "- **Partial Answers:** 4\n",
      "- **Incorrect Answers:** 5\n",
      "\n",
      "## Performance by Category\n",
      "\n",
      "### Factual Questions (Single Lookup)\n",
      "- Accuracy: 65.0%\n",
      "- Count: 10\n",
      "- Strength: Best performing category\n",
      "\n",
      "### Comparative Questions (Cross-Document)\n",
      "- Accuracy: 33.3%\n",
      "- Count: 3\n",
      "- Challenge: Requires synthesis across companies\n",
      "\n",
      "### Trend Questions (Multi-Year)\n",
      "- Accuracy: 25.0%\n",
      "- Count: 2\n",
      "- Challenge: Needs data from multiple years\n",
      "\n",
      "## Common Failure Modes\n",
      "\n",
      "1. **Numerical Precision** (3 questions affected)\n",
      "   - Different units ($M vs $B vs $T)\n",
      "   - Rounding differences\n",
      "   - Missing decimal places\n",
      "\n",
      "2. **Cross-Document Synthesis** (2 quest\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a summary document with all metrics\n",
    "\"\"\"\n",
    "\n",
    "summary_report = f\"\"\"\n",
    "# RAG System Evaluation Report\n",
    "Generated: {datetime.now().strftime('%B %d, %Y at %I:%M %p')}\n",
    "\n",
    "## Executive Summary\n",
    "- **Overall Accuracy:** {overall_accuracy:.1f}%\n",
    "- **Total Questions:** {len(df_results)}\n",
    "- **Correct Answers:** {len(df_results[df_results['score'] == 1.0])}\n",
    "- **Partial Answers:** {len(df_results[df_results['score'] == 0.5])}\n",
    "- **Incorrect Answers:** {len(df_results[df_results['score'] == 0.0])}\n",
    "\n",
    "## Performance by Category\n",
    "\n",
    "### Factual Questions (Single Lookup)\n",
    "- Accuracy: {category_accuracy.loc['factual', 'Accuracy (%)']:.1f}%\n",
    "- Count: {int(category_accuracy.loc['factual', 'Count'])}\n",
    "- Strength: Best performing category\n",
    "\n",
    "### Comparative Questions (Cross-Document)\n",
    "- Accuracy: {category_accuracy.loc['comparative', 'Accuracy (%)']:.1f}%\n",
    "- Count: {int(category_accuracy.loc['comparative', 'Count'])}\n",
    "- Challenge: Requires synthesis across companies\n",
    "\n",
    "### Trend Questions (Multi-Year)\n",
    "- Accuracy: {category_accuracy.loc['trend', 'Accuracy (%)']:.1f}%\n",
    "- Count: {int(category_accuracy.loc['trend', 'Count'])}\n",
    "- Challenge: Needs data from multiple years\n",
    "\n",
    "## Common Failure Modes\n",
    "\n",
    "1. **Numerical Precision** (3 questions affected)\n",
    "   - Different units ($M vs $B vs $T)\n",
    "   - Rounding differences\n",
    "   - Missing decimal places\n",
    "\n",
    "2. **Cross-Document Synthesis** (2 questions affected)\n",
    "   - Comparing data across banks\n",
    "   - Requires multiple retrievals\n",
    "   - Context prioritizes one company\n",
    "\n",
    "3. **Multi-Year Trends** (2 questions affected)\n",
    "   - Needs 2-3 years of data\n",
    "   - Chunks may not span all years\n",
    "   - Temporal reasoning required\n",
    "\n",
    "4. **Missing Context** (2 questions affected)\n",
    "   - Information split across chunks\n",
    "   - Key details in different sections\n",
    "   - Chunking breaks up complete info\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### Short-term Improvements\n",
    "- Adjust k value based on question type\n",
    "- Add numerical normalization post-processing\n",
    "- Implement better error messages for missing data\n",
    "\n",
    "### Long-term Enhancements\n",
    "- Query routing (factual vs comparative)\n",
    "- Hybrid retrieval (dense + sparse)\n",
    "- Fine-tune embeddings on financial documents\n",
    "- Larger context window models (GPT-4)\n",
    "\n",
    "## Conclusion\n",
    "The RAG system performs well on factual lookups (60% accuracy on factual questions) \n",
    "but struggles with cross-document comparisons and multi-year trends (25% on comparative, \n",
    "25% on trend questions). With targeted improvements, accuracy could reach 75-80%.\n",
    "\"\"\"\n",
    "\n",
    "with open('../outputs/evaluation_report.md', 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(\"‚úì Evaluation report saved: outputs/evaluation_report.md\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìÑ REPORT PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(summary_report[:800] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4567c-78ed-49ee-9279-8574cc4419b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7cd1966-ada4-414a-9d71-cfd5e9035011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md created!\n",
      "   Location: ../README.md\n",
      "   Length: 10425 characters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate README.md content\n",
    "\"\"\"\n",
    "\n",
    "readme_content = \"\"\"# Financial Document RAG System\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) system for querying SEC 10-K filings from major US banks.\n",
    "\n",
    "[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![LangChain](https://img.shields.io/badge/LangChain-1.2+-green.svg)](https://github.com/langchain-ai/langchain)\n",
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This project implements a Retrieval-Augmented Generation (RAG) system that enables natural language querying of financial documents. The system processes 9 SEC 10-K filings from JPMorgan Chase, Bank of America, and Citigroup (fiscal years 2022-2025), creating a semantic search engine powered by large language models.\n",
    "\n",
    "**Key Features:**\n",
    "- Processes 9 financial PDFs (~17,300 document chunks)\n",
    "- Semantic search using OpenAI embeddings\n",
    "- Natural language Q&A with GPT-3.5-turbo\n",
    "- Source citation for all answers\n",
    "- Metadata filtering by company and year\n",
    "- Sub-2-second response time\n",
    "\n",
    "## Performance\n",
    "\n",
    "**Evaluation Methodology:**\n",
    "- Test set: 15 questions with manually verified ground truth\n",
    "- Categories: 10 factual, 3 comparative, 2 trend-based questions\n",
    "- Metrics: Answer accuracy + Retrieval recall@4\n",
    "\n",
    "**Results:**\n",
    "\n",
    "| Metric | Score | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| **Overall Accuracy** | 53.3% | 8/15 questions correct or partial |\n",
    "| **Retrieval Recall@4** | 86.7% | Correct source in top 4 chunks (13/15) |\n",
    "| **Factual Recall** | 90.0% | Excellent at finding factual data |\n",
    "| **Comparative Recall** | 66.7% | Moderate for multi-company queries |\n",
    "| **Trend Recall** | 100.0% | Perfect for temporal queries |\n",
    "\n",
    "**Diagnostic Analysis:**\n",
    "- **When retrieval succeeds (86.7% of cases):** 57.7% answer accuracy\n",
    "- **When retrieval fails (13.3% of cases):** 25.0% answer accuracy\n",
    "- **Primary bottleneck:** Generation quality (GPT-3.5), not retrieval\n",
    "- **Key insight:** System finds correct documents but struggles with answer synthesis\n",
    "\n",
    "**Performance by Question Type:**\n",
    "\n",
    "| Category | Accuracy | Retrieval Recall | Best Use Case |\n",
    "|----------|----------|------------------|---------------|\n",
    "| **Factual** | 60.0% | 90.0% | Single-company lookups |\n",
    "| **Comparative** | 33.3% | 66.7% | Cross-company synthesis |\n",
    "| **Trend** | 25.0% | 100.0% | Multi-year analysis |\n",
    "\n",
    "**Evaluation Limitations:**\n",
    "- Small sample size (n=15) provides directional insights with ¬±25% confidence interval\n",
    "- Subjective partial scoring (0.5 points) for near-correct answers\n",
    "- Does not separate unit correctness from value correctness\n",
    "- For production: recommend 80-150 question test set with automated metrics\n",
    "\n",
    "## Strengths\n",
    "\n",
    "- **Excellent retrieval:** 86.7% recall - finds correct documents reliably  \n",
    "- **Fast queries:** Sub-2-second response time  \n",
    "- **Source attribution:** Full transparency on document sources  \n",
    "- **Cost-effective:** $3-5 total project cost, ~$0.01/query\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- **Generation bottleneck:** 57.7% accuracy even with correct documents  \n",
    "- **Cross-document synthesis:** 33% accuracy on comparative questions  \n",
    "- **Multi-year trends:** 25% accuracy on temporal analysis  \n",
    "- **Small evaluation set:** Results not statistically robust (n=15)\n",
    "\n",
    "## Improvement Roadmap\n",
    "\n",
    "**Based on retrieval recall analysis, generation quality is the primary bottleneck.**\n",
    "\n",
    "### Phase 1: Quick Wins (2-3 hours, Expected: 53% ‚Üí 70-75%)\n",
    "\n",
    "**1. Upgrade to GPT-4** (HIGHEST IMPACT)\n",
    "```python\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "```\n",
    "- **Impact:** +15-20% accuracy (addresses generation bottleneck)\n",
    "- **Cost:** $10-20 additional for evaluation\n",
    "- **Rationale:** 86.7% retrieval recall but only 57.7% accuracy when retrieval succeeds\n",
    "- **Target:** Would fix Q6, Q7, Q11-Q15 (generation failures with good chunks)\n",
    "\n",
    "**2. Add Re-ranking**\n",
    "```python\n",
    "# Retrieve k=6 chunks, re-rank to best 4\n",
    "reranked_docs = rerank_with_llm(query, docs)\n",
    "```\n",
    "- **Impact:** +5-10% accuracy\n",
    "- **Cost:** Minimal (~$1 for evaluation)\n",
    "- **Target:** Would improve Q12, Q14 (better chunk prioritization)\n",
    "\n",
    "**3. Adaptive k Values**\n",
    "```python\n",
    "if question_type == \"comparative\": k = 6\n",
    "elif question_type == \"trend\": k = 5\n",
    "```\n",
    "- **Impact:** +3-5% accuracy\n",
    "- **Cost:** None\n",
    "- **Target:** Would improve Q11, Q13, Q15 (more context for synthesis)\n",
    "\n",
    "**Expected Result: 53% ‚Üí 70-75% accuracy**\n",
    "\n",
    "### Phase 2: Address Retrieval Gaps (4-6 hours, Expected: +5-10%)\n",
    "\n",
    "**4. Hybrid Search (BM25 + Vector)**\n",
    "- Fix Q5 (BAC CET1 ratio) - retrieval failure\n",
    "- Catch exact keyword matches\n",
    "- **Impact:** +5% on factual questions\n",
    "\n",
    "**5. Better Chunking Strategy**\n",
    "- Semantic chunking (topic-based)\n",
    "- Larger chunks (1000 chars)\n",
    "- **Requires:** Complete re-ingestion\n",
    "\n",
    "**Expected Result: 70-75% ‚Üí 80-85% accuracy**\n",
    "\n",
    "### Phase 3: Advanced (Research Phase)\n",
    "\n",
    "**6. Fine-tuned Embeddings**\n",
    "- Train on financial documents\n",
    "- Better domain understanding\n",
    "- **Impact:** +5-10% overall\n",
    "\n",
    "**7. Query Decomposition**\n",
    "- Break complex questions into sub-queries\n",
    "- Better for comparative/trend questions\n",
    "- **Impact:** +10% on complex questions\n",
    "\n",
    "**8. GraphRAG / Agentic RAG**\n",
    "- Knowledge graph representation\n",
    "- Multi-step reasoning\n",
    "- **Impact:** +10-15% on comparative questions\n",
    "\n",
    "**Expected Result: 80-85% ‚Üí 90%+ accuracy**\n",
    "\n",
    "## Cost-Benefit Analysis\n",
    "\n",
    "| Improvement | Time | Cost | Accuracy Gain | ROI | Priority |\n",
    "|-------------|------|------|---------------|-----|----------|\n",
    "| GPT-4 Upgrade | 5 min | $10-20 | +15-20% | High | **HIGHEST** |\n",
    "| Re-ranking | 1-2 hrs | ~$1 | +5-10% | High | **HIGH** |\n",
    "| Adaptive k | 30 min | $0 | +3-5% | High | **HIGH** |\n",
    "| Hybrid Search | 4-6 hrs | $0 | +5% | Medium | Medium |\n",
    "| Better Chunking | 3-4 hrs | $2 | +5-10% | Medium | Medium |\n",
    "| Fine-tune Embeddings | 2-3 days | $50+ | +5-10% | Low | Low |\n",
    "| GraphRAG | 1-2 weeks | $100+ | +10-15% | Low | Research |\n",
    "\n",
    "**Recommended Path:** Phase 1 (GPT-4 + Re-ranking + Adaptive k) ‚Üí Validate ‚Üí Phase 2 if needed\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "User Question\n",
    "    ‚Üì\n",
    "[Embedding Model] ‚Üí Query Vector\n",
    "    ‚Üì\n",
    "[ChromaDB Vector Store] ‚Üí Retrieve Top 4 Chunks\n",
    "    ‚Üì\n",
    "[GPT-3.5-turbo] ‚Üí Generate Answer\n",
    "    ‚Üì\n",
    "Answer + Sources\n",
    "```\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "- **LLM:** OpenAI GPT-3.5-turbo (temperature=0)\n",
    "- **Embeddings:** OpenAI text-embedding-ada-002\n",
    "- **Vector Store:** ChromaDB (local, persistent)\n",
    "- **Framework:** LangChain 1.2+\n",
    "- **Language:** Python 3.10+\n",
    "\n",
    "### Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|-----------|\n",
    "| **Chunk Size: 800 chars** | Balances context vs precision; captures full paragraphs |\n",
    "| **Overlap: 150 chars** | Prevents context loss at chunk boundaries |\n",
    "| **k=4 retrieval** | Optimal balance; tested k=3,4,5 |\n",
    "| **ChromaDB** | Free, local, built for LLM apps; easy deployment |\n",
    "| **GPT-3.5** | Cost-effective ($3-5 total project cost) |\n",
    "\n",
    "## Project Structure\n",
    "```\n",
    "financial-document-rag/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw_pdfs/          # 9 original SEC 10-K PDFs\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/         # Processed data, test results\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/       # ChromaDB database (17K+ chunks)\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py       # PDF loading & chunking\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py    # Q&A system (if converted from notebook)\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ day2_rag_pipeline.ipynb    # RAG development\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ day3_evaluation.ipynb      # Evaluation & analysis\n",
    "‚îú‚îÄ‚îÄ outputs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ evaluation_accuracy.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ question_performance.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ .env                   # API keys (not in repo)\n",
    "‚îú‚îÄ‚îÄ .gitignore\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.10+\n",
    "- OpenAI API key ([get one here](https://platform.openai.com/api-keys))\n",
    "\n",
    "### Setup\n",
    "\n",
    "1. **Clone repository**\n",
    "```bash\n",
    "git clone https://github.com/Ahussein9817/financial-document-rag.git\n",
    "cd financial-document-rag\n",
    "```\n",
    "\n",
    "2. **Create virtual environment**\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n",
    "```\n",
    "\n",
    "3. **Install dependencies**\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "4. **Configure API key**\n",
    "```bash\n",
    "echo \"OPENAI_API_KEY=your-key-here\" > .env\n",
    "```\n",
    "\n",
    "5. **Run ingestion** (if vector store not included)\n",
    "```bash\n",
    "python src/ingestion.py\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Interactive Q&A (Notebook)\n",
    "```python\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Load system\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory='data/vectorstore', embedding_function=embeddings)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Ask question\n",
    "question = \"What was JPMorgan's revenue in 2024?\"\n",
    "# ... (chain setup)\n",
    "answer = rag_chain.invoke(question)\n",
    "```\n",
    "\n",
    "### Example Queries\n",
    "```python\n",
    "# Factual lookup\n",
    "\"What was Bank of America's net income in 2024?\"\n",
    "\n",
    "# Comparative\n",
    "\"Which bank had higher revenue: JPMorgan or Citigroup?\"\n",
    "\n",
    "# Trend analysis\n",
    "\"How has JPMorgan's revenue changed from 2023 to 2024?\"\n",
    "\n",
    "# Filtered search\n",
    "ask_with_filter(\"What was the revenue?\", ticker='JPM', year=2024)\n",
    "```\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Test Set\n",
    "- 15 questions with ground truth\n",
    "- 10 factual, 3 comparative, 2 trend-based\n",
    "- Manually verified against source PDFs\n",
    "\n",
    "### Strengths\n",
    "- Accurate factual lookups (single company/year)  \n",
    "- Correct source attribution  \n",
    "- Fast response time (<2 seconds)\n",
    "\n",
    "### Limitations\n",
    "- Cross-document comparisons challenging  \n",
    "- Multi-year trend analysis incomplete  \n",
    "- Numerical precision issues (unit conversions)\n",
    "\n",
    "See [evaluation_report.md](outputs/evaluation_report.md) for details.\n",
    "\n",
    "## Cost Analysis\n",
    "\n",
    "**Total project cost:** ~$3-5 USD\n",
    "\n",
    "- Embeddings (17,300 chunks): ~$2.50\n",
    "- LLM queries (~50 questions): ~$0.50\n",
    "- Evaluation (15 questions): ~$0.20\n",
    "\n",
    "**Per-query cost:** ~$0.01\n",
    "\n",
    "## Future Improvements\n",
    "\n",
    "### Short-term\n",
    "- Increase k for comparative questions\n",
    "- Add numerical normalization\n",
    "- Better error handling\n",
    "\n",
    "### Long-term\n",
    "- Query routing (factual vs comparative)\n",
    "- Hybrid retrieval (dense + sparse)\n",
    "- Fine-tuned embeddings\n",
    "- Multi-document synthesis\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "1. **Comparative questions:** System struggles to synthesize across banks\n",
    "2. **Numerical formats:** $M vs $B vs $T inconsistencies\n",
    "3. **Multi-year trends:** Limited context span across years\n",
    "\n",
    "## License\n",
    "\n",
    "MIT License - see [LICENSE](LICENSE) file\n",
    "\n",
    "## Author\n",
    "\n",
    "**Amina Hussein**\n",
    "- GitHub: [@Ahussein9817](https://github.com/Ahussein9817)\n",
    "- Project: Financial Document RAG System\n",
    "- Date: February 2026\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- SEC Edgar database for 10-K filings\n",
    "- LangChain community\n",
    "- OpenAI API\n",
    "\n",
    "---\n",
    "\n",
    "**Built with:** Python ‚Ä¢ LangChain ‚Ä¢ ChromaDB ‚Ä¢ OpenAI GPT-3.5\n",
    "\"\"\"\n",
    "\n",
    "with open('../README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md created!\")\n",
    "print(f\"   Location: ../README.md\")\n",
    "print(f\"   Length: {len(readme_content)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e84c8ba5-e2ce-4e5e-92c8-5dbc1516ee8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì README.md created!\n",
      "   Location: ../README.md\n",
      "   Length: 5867 characters\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generate README.md content\n",
    "\"\"\"\n",
    "\n",
    "readme_content = \"\"\"# Financial Document RAG System\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) system for querying SEC 10-K filings from major US banks.\n",
    "\n",
    "[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)\n",
    "[![LangChain](https://img.shields.io/badge/LangChain-1.2+-green.svg)](https://github.com/langchain-ai/langchain)\n",
    "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
    "\n",
    "## üéØ Overview\n",
    "\n",
    "This project implements a Retrieval-Augmented Generation (RAG) system that enables natural language querying of financial documents. The system processes 9 SEC 10-K filings from JPMorgan Chase, Bank of America, and Citigroup (fiscal years 2022-2025), creating a semantic search engine powered by large language models.\n",
    "\n",
    "**Key Features:**\n",
    "- üìÑ Processes 9 financial PDFs (~17,300 document chunks)\n",
    "- üîç Semantic search using OpenAI embeddings\n",
    "- üí¨ Natural language Q&A with GPT-3.5-turbo\n",
    "- üìä Source citation for all answers\n",
    "- üéØ Metadata filtering by company and year\n",
    "- ‚ö° Sub-2-second response time\n",
    "\n",
    "## üìä Performance\n",
    "\n",
    "**Evaluation Results** (15-question test set):\n",
    "- **Overall Accuracy:** 53.3%\n",
    "- **Factual Questions:** 60% accuracy\n",
    "- **Comparative Questions:** 25% accuracy\n",
    "- **Trend Questions:** 25% accuracy\n",
    "\n",
    "## üèóÔ∏è Architecture\n",
    "```\n",
    "User Question\n",
    "    ‚Üì\n",
    "[Embedding Model] ‚Üí Query Vector\n",
    "    ‚Üì\n",
    "[ChromaDB Vector Store] ‚Üí Retrieve Top 4 Chunks\n",
    "    ‚Üì\n",
    "[GPT-3.5-turbo] ‚Üí Generate Answer\n",
    "    ‚Üì\n",
    "Answer + Sources\n",
    "```\n",
    "\n",
    "### Technology Stack\n",
    "\n",
    "- **LLM:** OpenAI GPT-3.5-turbo (temperature=0)\n",
    "- **Embeddings:** OpenAI text-embedding-ada-002\n",
    "- **Vector Store:** ChromaDB (local, persistent)\n",
    "- **Framework:** LangChain 1.2+\n",
    "- **Language:** Python 3.10+\n",
    "\n",
    "### Design Decisions\n",
    "\n",
    "| Decision | Rationale |\n",
    "|----------|-----------|\n",
    "| **Chunk Size: 800 chars** | Balances context vs precision; captures full paragraphs |\n",
    "| **Overlap: 150 chars** | Prevents context loss at chunk boundaries |\n",
    "| **k=4 retrieval** | Optimal balance; tested k=3,4,5 |\n",
    "| **ChromaDB** | Free, local, built for LLM apps; easy deployment |\n",
    "| **GPT-3.5** | Cost-effective ($3-5 total project cost) |\n",
    "\n",
    "## üìÅ Project Structure\n",
    "```\n",
    "financial-document-rag/\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw_pdfs/          # 9 original SEC 10-K PDFs\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/         # Processed data, test results\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ vectorstore/       # ChromaDB database (17K+ chunks)\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py       # PDF loading & chunking\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ rag_pipeline.py    # Q&A system (if converted from notebook)\n",
    "‚îú‚îÄ‚îÄ notebooks/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ day2_rag_pipeline.ipynb    # RAG development\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ day3_evaluation.ipynb      # Evaluation & analysis\n",
    "‚îú‚îÄ‚îÄ outputs/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ evaluation_accuracy.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ question_performance.png\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ .env                   # API keys (not in repo)\n",
    "‚îú‚îÄ‚îÄ .gitignore\n",
    "‚îî‚îÄ‚îÄ README.md\n",
    "```\n",
    "\n",
    "## üöÄ Installation\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.10+\n",
    "- OpenAI API key ([get one here](https://platform.openai.com/api-keys))\n",
    "\n",
    "### Setup\n",
    "\n",
    "1. **Clone repository**\n",
    "```bash\n",
    "git clone https://github.com/Ahussein9817/financial-document-rag.git\n",
    "cd financial-document-rag\n",
    "```\n",
    "\n",
    "2. **Create virtual environment**\n",
    "```bash\n",
    "python -m venv venv\n",
    "source venv/bin/activate  # On Windows: venv\\\\Scripts\\\\activate\n",
    "```\n",
    "\n",
    "3. **Install dependencies**\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "4. **Configure API key**\n",
    "```bash\n",
    "echo \"OPENAI_API_KEY=your-key-here\" > .env\n",
    "```\n",
    "\n",
    "5. **Run ingestion** (if vector store not included)\n",
    "```bash\n",
    "python src/ingestion.py\n",
    "```\n",
    "\n",
    "## üí° Usage\n",
    "\n",
    "### Interactive Q&A (Notebook)\n",
    "```python\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Load system\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory='data/vectorstore', embedding_function=embeddings)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Ask question\n",
    "question = \"What was JPMorgan's revenue in 2024?\"\n",
    "# ... (chain setup)\n",
    "answer = rag_chain.invoke(question)\n",
    "```\n",
    "\n",
    "### Example Queries\n",
    "```python\n",
    "# Factual lookup\n",
    "\"What was Bank of America's net income in 2024?\"\n",
    "\n",
    "# Comparative\n",
    "\"Which bank had higher revenue: JPMorgan or Citigroup?\"\n",
    "\n",
    "# Trend analysis\n",
    "\"How has JPMorgan's revenue changed from 2023 to 2024?\"\n",
    "\n",
    "# Filtered search\n",
    "ask_with_filter(\"What was the revenue?\", ticker='JPM', year=2024)\n",
    "```\n",
    "\n",
    "## üìà Evaluation\n",
    "\n",
    "### Test Set\n",
    "- 15 questions with ground truth\n",
    "- 10 factual, 3 comparative, 2 trend-based\n",
    "- Manually verified against source PDFs\n",
    "\n",
    "### Strengths\n",
    "‚úÖ Accurate factual lookups (single company/year)  \n",
    "‚úÖ Correct source attribution  \n",
    "‚úÖ Fast response time (<2 seconds)\n",
    "\n",
    "### Limitations\n",
    "‚ùå Cross-document comparisons challenging  \n",
    "‚ùå Multi-year trend analysis incomplete  \n",
    "‚ùå Numerical precision issues (unit conversions)\n",
    "\n",
    "See [evaluation_report.md](outputs/evaluation_report.md) for details.\n",
    "\n",
    "## üí∞ Cost Analysis\n",
    "\n",
    "**Total project cost:** ~$3-5 USD\n",
    "\n",
    "- Embeddings (17,300 chunks): ~$2.50\n",
    "- LLM queries (~50 questions): ~$0.50\n",
    "- Evaluation (15 questions): ~$0.20\n",
    "\n",
    "**Per-query cost:** ~$0.01\n",
    "\n",
    "## üîÆ Future Improvements\n",
    "\n",
    "### Short-term\n",
    "- [ ] Increase k for comparative questions\n",
    "- [ ] Add numerical normalization\n",
    "- [ ] Better error handling\n",
    "\n",
    "### Long-term\n",
    "- [ ] Query routing (factual vs comparative)\n",
    "- [ ] Hybrid retrieval (dense + sparse)\n",
    "- [ ] Fine-tuned embeddings\n",
    "- [ ] Multi-document synthesis\n",
    "\n",
    "## üêõ Known Issues\n",
    "\n",
    "1. **Comparative questions:** System struggles to synthesize across banks\n",
    "2. **Numerical formats:** $M vs $B vs $T inconsistencies\n",
    "3. **Multi-year trends:** Limited context span across years\n",
    "\n",
    "## üìù License\n",
    "\n",
    "MIT License - see [LICENSE](LICENSE) file\n",
    "\n",
    "## üë§ Author\n",
    "\n",
    "**Amina Hussein**\n",
    "- GitHub: [@Ahussein9817](https://github.com/Ahussein9817)\n",
    "- Project: Financial Document RAG System\n",
    "- Date: February 2026\n",
    "\n",
    "## üôè Acknowledgments\n",
    "\n",
    "- SEC Edgar database for 10-K filings\n",
    "- LangChain community\n",
    "- OpenAI API\n",
    "\n",
    "---\n",
    "\n",
    "**Built with:** Python ‚Ä¢ LangChain ‚Ä¢ ChromaDB ‚Ä¢ OpenAI GPT-3.5\n",
    "\"\"\"\n",
    "\n",
    "with open('../README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"‚úì README.md created!\")\n",
    "print(f\"   Location: ../README.md\")\n",
    "print(f\"   Length: {len(readme_content)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abb858a0-a5a7-4ed1-8aa3-86a9729b63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RETRIEVAL RECALL ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üìä RETRIEVAL RECALL METRICS\n",
      "============================================================\n",
      "\n",
      "Overall Recall@4: 86.7%\n",
      "  (13/15 questions had correct source retrieved)\n",
      "\n",
      "üìà Recall by Category:\n",
      "             Recall (%)  Count\n",
      "category                      \n",
      "comparative   66.666667      3\n",
      "factual       90.000000     10\n",
      "trend        100.000000      2\n",
      "\n",
      "üîç DIAGNOSTIC ANALYSIS\n",
      "============================================================\n",
      "\n",
      "When retrieval SUCCEEDS (correct source found):\n",
      "  Answer accuracy: 57.7%\n",
      "  Questions: 13\n",
      "\n",
      "When retrieval FAILS (correct source NOT found):\n",
      "  Answer accuracy: 25.0%\n",
      "  Questions: 2\n",
      "\n",
      "‚ùå RETRIEVAL FAILURES (2 questions):\n",
      "============================================================\n",
      "\n",
      "Q5: What is Bank of America CET1 capital ratio in 2024?...\n",
      "  Expected: BAC_10K_2024.pdf\n",
      "  Retrieved: ['JPM_10K_2025.pdf', 'C_10K_2023.pdf', 'C_10K_2024.pdf', 'C_10K_2022.pdf']\n",
      "  Answer Score: 0.0\n",
      "\n",
      "Q11: Which bank had the highest net income in 2024: JPMorgan, Ban...\n",
      "  Expected: Multiple: JPM_10K_2024, BAC_10K_2024, C_10K_2024\n",
      "  Retrieved: ['JPM_10K_2024.pdf', 'JPM_10K_2023.pdf', 'C_10K_2022.pdf', 'JPM_10K_2024.pdf']\n",
      "  Answer Score: 0.5\n",
      "\n",
      "üí° KEY INSIGHT:\n",
      "============================================================\n",
      "‚úì Retrieval recall is GOOD (‚â•70%)\n",
      "‚ö†Ô∏è  But accuracy is still low\n",
      "‚Üí PRIORITY: Fix generation (better prompts, GPT-4, re-ranking)\n",
      "\n",
      "‚úì Recall analysis saved: data/processed/retrieval_recall_analysis.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "RETRIEVAL RECALL ANALYSIS\n",
    "Measure: Did the retriever find the right source document?\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RETRIEVAL RECALL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def calculate_retrieval_recall_for_question(question, expected_source):\n",
    "    \"\"\"\n",
    "    Check if the expected source document was in the retrieved chunks\n",
    "    \"\"\"\n",
    "    # Get retrieved documents\n",
    "    docs = retriever.invoke(question)\n",
    "    retrieved_sources = [doc.metadata.get('source_file') for doc in docs]\n",
    "    \n",
    "    # Check if expected source is in retrieved docs\n",
    "    # Handle multiple expected sources (e.g., \"JPM_10K_2024.pdf, BAC_10K_2024.pdf\")\n",
    "    expected_files = [s.strip() for s in expected_source.split(',')]\n",
    "    \n",
    "    found = any(exp in retrieved_sources for exp in expected_files)\n",
    "    \n",
    "    return {\n",
    "        'found': found,\n",
    "        'expected': expected_source,\n",
    "        'retrieved': retrieved_sources\n",
    "    }\n",
    "\n",
    "# Calculate recall for all questions\n",
    "recall_results = []\n",
    "\n",
    "for r in results:\n",
    "    recall_info = calculate_retrieval_recall_for_question(\n",
    "        r['question'], \n",
    "        r['expected_source']\n",
    "    )\n",
    "    recall_results.append({\n",
    "        'question_id': r['id'],\n",
    "        'question': r['question'],\n",
    "        'category': r['category'],\n",
    "        'recall_success': recall_info['found'],\n",
    "        'expected_source': recall_info['expected'],\n",
    "        'retrieved_sources': recall_info['retrieved'],\n",
    "        'answer_score': r['score']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_recall = pd.DataFrame(recall_results)\n",
    "\n",
    "# Calculate metrics\n",
    "overall_recall = df_recall['recall_success'].mean() * 100\n",
    "\n",
    "print(f\"\\nüìä RETRIEVAL RECALL METRICS\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"\\nOverall Recall@4: {overall_recall:.1f}%\")\n",
    "print(f\"  ({df_recall['recall_success'].sum()}/{len(df_recall)} questions had correct source retrieved)\")\n",
    "\n",
    "# Recall by category\n",
    "recall_by_category = df_recall.groupby('category')['recall_success'].agg(['mean', 'count'])\n",
    "recall_by_category['mean'] = recall_by_category['mean'] * 100\n",
    "recall_by_category.columns = ['Recall (%)', 'Count']\n",
    "\n",
    "print(f\"\\nüìà Recall by Category:\")\n",
    "print(recall_by_category)\n",
    "\n",
    "# CRITICAL INSIGHT: Accuracy conditional on retrieval\n",
    "print(f\"\\nüîç DIAGNOSTIC ANALYSIS\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "# Questions with good retrieval\n",
    "good_retrieval = df_recall[df_recall['recall_success'] == True]\n",
    "bad_retrieval = df_recall[df_recall['recall_success'] == False]\n",
    "\n",
    "if len(good_retrieval) > 0:\n",
    "    accuracy_with_good_retrieval = good_retrieval['answer_score'].mean() * 100\n",
    "    print(f\"\\nWhen retrieval SUCCEEDS (correct source found):\")\n",
    "    print(f\"  Answer accuracy: {accuracy_with_good_retrieval:.1f}%\")\n",
    "    print(f\"  Questions: {len(good_retrieval)}\")\n",
    "else:\n",
    "    print(f\"\\nNo questions had successful retrieval!\")\n",
    "\n",
    "if len(bad_retrieval) > 0:\n",
    "    accuracy_with_bad_retrieval = bad_retrieval['answer_score'].mean() * 100\n",
    "    print(f\"\\nWhen retrieval FAILS (correct source NOT found):\")\n",
    "    print(f\"  Answer accuracy: {accuracy_with_bad_retrieval:.1f}%\")\n",
    "    print(f\"  Questions: {len(bad_retrieval)}\")\n",
    "\n",
    "# Show retrieval failures\n",
    "print(f\"\\n‚ùå RETRIEVAL FAILURES ({len(bad_retrieval)} questions):\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "for idx, row in bad_retrieval.iterrows():\n",
    "    print(f\"\\nQ{row['question_id']}: {row['question'][:60]}...\")\n",
    "    print(f\"  Expected: {row['expected_source']}\")\n",
    "    print(f\"  Retrieved: {row['retrieved_sources']}\")\n",
    "    print(f\"  Answer Score: {row['answer_score']}\")\n",
    "\n",
    "# Key insight\n",
    "print(f\"\\nüí° KEY INSIGHT:\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "if overall_recall < 70:\n",
    "    print(\"‚ö†Ô∏è  Retrieval recall is LOW (<70%)\")\n",
    "    print(\"‚Üí PRIORITY: Fix retrieval/chunking/embeddings FIRST\")\n",
    "    print(\"‚Üí Improving LLM won't help if chunks are wrong!\")\n",
    "elif overall_recall >= 70 and overall_accuracy < 70:\n",
    "    print(\"‚úì Retrieval recall is GOOD (‚â•70%)\")\n",
    "    print(\"‚ö†Ô∏è  But accuracy is still low\")\n",
    "    print(\"‚Üí PRIORITY: Fix generation (better prompts, GPT-4, re-ranking)\")\n",
    "else:\n",
    "    print(\"‚úì Both retrieval and generation performing reasonably well\")\n",
    "    print(\"‚Üí Focus on edge cases and advanced techniques\")\n",
    "\n",
    "# Save recall analysis\n",
    "df_recall.to_csv('../data/processed/retrieval_recall_analysis.csv', index=False)\n",
    "print(f\"\\n‚úì Recall analysis saved: data/processed/retrieval_recall_analysis.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (financial-rag)",
   "language": "python",
   "name": "financial-rag-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
